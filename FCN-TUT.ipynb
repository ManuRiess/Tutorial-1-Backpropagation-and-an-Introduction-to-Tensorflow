{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "# Deep Learning Foundations\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to get a first understanding of neural networks and the tensorflow library. You will get an understanding of backpropagation, convolutional layers, fully connected layers, activation functions, loss functions, and bring all this together into your first neural network architecture.\n",
    "\n",
    "<img src=\"graphics/set_sails.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: Setting sails for the deep learning journey. Image from [pixabay](https://pixabay.com/de/photos/) </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Backpropagation is the central part of iterative optimization. The backpropagation utilizes the derivative of each unit in the neural network starting with the determined error resulting from the loss function. With the chain rule the various units are connected to obtain the derivatives of the loss function through all layers. The resulting matrix shows you how each weight effects the output and its error. Thus, aiming to minimize the error the derivatives tell you in which direction to update the associated weights.\n",
    "\n",
    "\n",
    "Define an input vector x with two entries valued (0.2 and 0.4) and a fully connected weight matrix W with two units. The groundtruth should be defined as 1.\n",
    "\n",
    "\\begin{align}\n",
    "\tx =\\begin{bmatrix}\n",
    "\t0.2 \\\\ 0.4 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\\begin{align}\n",
    "\ty = 1\n",
    "\\end{align}\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first do some experiments in numpy, no worries there will be enough tensorflow soon.\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[0.2], [2]])\n",
    "W = np.array([[-0.3, 0.8]])\n",
    "y = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our architecture to reach the prediction of our one unit neural network, we split the equation in two functions so we can define the derivates partially in the next step:\n",
    "\n",
    "\\begin{align}\n",
    "\t f(x,W)= || W*x ||^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication(W,x):\n",
    "    q = '''implement a scalar multiplication with numpy'''\n",
    "    return q\n",
    "\n",
    "def prediction(q):\n",
    "    f_1 = '''implement the square function in two steps'''\n",
    "    y_pred = '''implement the square function in two steps'''\n",
    "   \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the error we need to specify a loss function or objective function and our weight update step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update (W, grad_W, learning_rate):\n",
    "    W = '''implement the weight update step'''\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_loss(y_pred, y):\n",
    "    error = '''implement the error'''\n",
    "    loss = '''implement the squared error loss function'''\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to determine the partial derivatives of our neural network. We aim to determine the gradient of our function \n",
    "$f(x,W)$ dependent from the input values of our network $x$, and the networks parameters $W$.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(x,W)}{\\partial W}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(x,W)}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "By using the chain rule we can easily propagate the error through the local derivatives at each operation and unit.\n",
    "\n",
    "The derivative of the last stage of our unit can be written as: \n",
    "\\begin{align}\n",
    "\tf(q)= ||q||^2 = q_{1}^2+...+q_{n}^2, \\frac{\\partial f(q)}{\\partial q} = 2*q_{i}.\n",
    "\\end{align}\n",
    "\n",
    "For the next node we have two variable inputs and so we have to calculate the local derivatives with respect to both $W$, since this is where we can adjust the weights.\n",
    "\\begin{align}\n",
    "\tq = W*x\t\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial q_{k}}{\\partial W_{i,j}}=x\n",
    "\\end{align}\n",
    "\n",
    "With the chain rule, we can now backpropagate the error to those weights:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(x,W)}{\\partial W} = \\frac{\\partial f(q)}{\\partial q} * \\frac{\\partial q}{\\partial W} = 2*q*x^{T}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the loss function\n",
    "def gradient_loss(y_pred, y):\n",
    "    '''implement the derivative at the loss function'''\n",
    "    return grad_loss\n",
    "\n",
    "# Over the square operator\n",
    "def gradient_prediction (q, grad_loss):\n",
    "    grad_q = '''implement the derivative at the unit'''\n",
    "    return grad_q\n",
    "\n",
    "# Finally to the multiplication of our inputs\n",
    "def gradient_multiplication (x, grad_q):\n",
    "    if len(x) == 1:\n",
    "        grad_W = '''implement the derivative at the input'''\n",
    "    else:\n",
    "        grad_W = '''implement the derivative at the input'''\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we bring everything together in our iterative weight update process, in our optimization routine for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "weightlist1 = []\n",
    "weightlist2 = []\n",
    "predictionlist = []\n",
    "groundtruthlist = []\n",
    "\n",
    "for t in range(10):\n",
    "    \n",
    "    weightlist1.append(W[0][0])\n",
    "    weightlist2.append(W[0][1])\n",
    "    \n",
    "    # Forward pass\n",
    "    q = '''with the functions above implement the forward pass'''\n",
    "    y_pred = '''with the functions above implement the forward pass'''\n",
    "    loss = '''with the functions above implement the forward pass'''\n",
    "    print(\"Training loss: \", loss)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad_loss = '''with the functions above implement the backward pass'''\n",
    "    grad_q = '''with the functions above implement the backward pass'''\n",
    "    grad_W = '''with the functions above implement the backward pass'''\n",
    "    W = '''with the functions above implement the backward pass'''\n",
    "    predictionlist.append(y_pred)\n",
    "    groundtruthlist.append(y)\n",
    "    print(\"Current prediction: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How do the weights change with respect to the given input? Elaborate?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(weightlist1, weightlist2)\n",
    "plt.xlabel('Weight 1')\n",
    "plt.ylabel('Weight 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(predictionlist, groundtruthlist)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Groundtruth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow bring it on!\n",
    "Now we are going to get our hands dirty with TensorFlow, where most of the hard work is already done for you. This way we can finally focus on \"adding layers\" and \"going deeper\".\n",
    "\n",
    "## Optimization\n",
    "First we concentrate on optimization, this is basically mirroring what we did with numpy a few cell above. This time we will implement a full neural network unit, including an activation function and also we are adding a third input, introducing a bias for the unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# For reproducability\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network architecture\n",
    "n_input = 3\n",
    "n_output = 1\n",
    "n_units = 1\n",
    "\n",
    "# Training parameters\n",
    "n_updates = 10\n",
    "\n",
    "\n",
    "# Define graph / network\n",
    "weights = {\n",
    "    'h1': tf.Variable(np.reshape([np.float32(2.0), np.float32(2.0), np.float32(2.0)], (3, 1))),\n",
    "    # 'h1': tf.Variable(tf.random_normal([n_input, n_units])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(np.reshape([np.float32(4.0)], (1, 1))),\n",
    "    # 'b1': tf.Variable(tf.random_normal([n_units])),\n",
    "}\n",
    "\n",
    "# This is where we design our unit\n",
    "class unit(tf.keras.Model):\n",
    "    def __init__(self, c_weights, c_biases):\n",
    "        super(unit, self).__init__(name='')\n",
    "        self.c_weights = c_weights\n",
    "        self.c_biases = c_biases\n",
    "\n",
    "    def call(self, x0):\n",
    "        # unit / neuron structure\n",
    "        layer_1 = tf.add(tf.matmul(tf.cast(x0, tf.float32), self.c_weights['h1']), self.c_biases['b1'])\n",
    "        # activation function\n",
    "        y_pred = tf.nn.relu(layer_1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Input\n",
    "x = tf.Variable(np.reshape([1.0, 1.0, 3.0], (1, 3)))\n",
    "# predicted output\n",
    "y_pred = unit(weights, biases)\n",
    "# Expected output\n",
    "y_gt = tf.Variable(np.reshape([10.0], (1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.losses.mean_squared_error()\n",
    "\n",
    "# 1. Stochastic gradient descent\n",
    "# opt = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "\n",
    "# 2. Momentum optimizer\n",
    "# opt = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.75)\n",
    "\n",
    "# 3. Adaptive momentum optimizer\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "\n",
    "# 4. Adaptive momentum optimizer with adjusted initial learning rate\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a GradientTape.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass.\n",
    "    logits = y_pred(x)\n",
    "    # Loss value for this batch.\n",
    "    loss_value = cost(y_gt, logits)\n",
    "\n",
    "# Get gradients of loss wrt the weights.\n",
    "gradients = tape.gradient(loss_value, y_pred.trainable_weights)\n",
    "\n",
    "# Update the weights of the model.\n",
    "opt.apply_gradients(zip(gradients, y_pred.trainable_weights))\n",
    "\n",
    "\n",
    "print(y_pred.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the different backpropagation approaches and see whether you can reproduce the findings presented in the lecture.\n",
    "\n",
    "Notice the design of tensorflow, which first designs the model and then calls a session where the placeholders are filled and the forward and backward pass are executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Next we will get an intuition for regularization approaches (Evaluate the different designs 0. to 2.)  Also we are working with the Keras library, which is based on TensorFlow and introduces a more pythonic feeling. Do you have a notion why I would say so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "\n",
    "  # Design 0. without regularization:\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "\n",
    "  # Design 1. L2 Parameter norm penalty by kernel regularizer:\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "\n",
    "  # Design 2. Dropout:\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters, feel free to play with the different optimizers as well.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load training data (reduce training data to 10k samples)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_validation, y_validation) = mnist.load_data()\n",
    "\n",
    "# Normalize input images (comply with activation function)\n",
    "x_train, x_validation = x_train / 255.0, x_validation / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Augmentation\n",
    "# x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "# datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "#                                                                 featurewise_center=True,\n",
    "#                                                                 featurewise_std_normalization=True,\n",
    "#                                                                 rotation_range=20,\n",
    "#                                                                 width_shift_range=0.2,\n",
    "#                                                                 height_shift_range=0.2,\n",
    "#                                                                 horizontal_flip=False\n",
    "#                                                                 )\n",
    "#\n",
    "# for e in range(10):\n",
    "#     print('Epoch', e)\n",
    "#     batches = 0\n",
    "#     for x_batch, y_batch in datagenerator.flow(x_train, y_train, batch_size=32):\n",
    "#         model.fit(np.reshape(x_batch, (-1, 28, 28)), y_batch, shuffle=True)\n",
    "#         batches += 1\n",
    "#         if batches >= len(x_train) / 32:\n",
    "#             # we need to break the loop by hand because\n",
    "#             # the generator loops indefinitely\n",
    "#             break\n",
    "\n",
    "# 4. Early stopping (usually you should monitor the validation accuracy)\n",
    "# es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                                      min_delta=0,\n",
    "#                                      patience=1,\n",
    "#                                      mode='auto'\n",
    "#                                      )\n",
    "#\n",
    "# Fit model on training data (with callback)\n",
    "# model.fit(x_train, \n",
    "#          y_train, \n",
    "#          epochs=10, \n",
    "#          shuffle=True, \n",
    "#          callbacks=[es],\n",
    "#          validation_data=(x_validation, y_validation)\n",
    "#         )\n",
    "\n",
    "# Fit model on training data (without regularization)\n",
    "model.fit(x_train, y_train, epochs=10, shuffle=True)\n",
    "\n",
    "# Evaluate performance on validation set\n",
    "_, validation_acc = model.evaluate(x_validation, y_validation)\n",
    "print('validation accuracy:', validation_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Keep in mind that [MNIST](http://yann.lecun.com/exdb/mnist/) is the \"Hello world\" of machine learning, this is the go-to-dataset if you are in need for a toy problem. Most new ideas in machine learning are presented by the aid of MNIST, make sure that you familiarize yourself with the dataset and its characteristics.\n",
    "- Try different regularization approaches and their influence.\n",
    "- Can you think of a good task to introduce Convolutional Neural Networks, including receptive field, strides, semantic information, and so on? (Send your jupyter notebook to mark.schutera@kit.edu for a chance to earn bonus points).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
