{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning Tutorial \n",
    "contact: Mark.schutera@kit.edu\n",
    "# Deep Learning Foundations\n",
    "\n",
    "## Introduction\n",
    "In this tutorial, you will attempt to get a first understanding of neural networks and the tensorflow library. You will get an understanding of backpropagation, convolutional layers, fully connected layers, activation functions, loss functions, and bring all this together into your first neural network architecture.\n",
    "\n",
    "<img src=\"graphics/set_sails.jpg\" width=\"700\"><br>\n",
    "<center> Fig. 1: Setting sails for the deep learning journey. Image from [pixabay](https://pixabay.com/de/photos/) </center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation\n",
    "\n",
    "Backpropagation is the central part of iterative optimization. The backpropagation utilizes the derivative of each unit in the neural network starting with the determined error resulting from the loss function. With the chain rule the various units are connected to obtain the derivatives of the loss function through all layers. The resulting matrix shows you how each weight effects the output and its error. Thus, aiming to minimize the error the derivatives tell you in which direction to update the associated weights.\n",
    "\n",
    "\n",
    "Define an input vector x with two entries valued (0.2 and 0.4) and a fully connected weight matrix W with two units. The groundtruth should be defined as 1.\n",
    "\n",
    "\\begin{align}\n",
    "\tx =\\begin{bmatrix}\n",
    "\t0.2 \\\\ 0.4 \n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\t\n",
    "\n",
    "\\begin{align}\n",
    "\ty = 1\n",
    "\\end{align}\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will first do some experiments in numpy, no worries there will be enough tensorflow soon.\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[0.2], [2]])\n",
    "W = np.array([[-0.3, 0.8]])\n",
    "y = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define our architecture to reach the prediction of our one unit neural network, we split the equation in two functions so we can define the derivates partially in the next step:\n",
    "\n",
    "\\begin{align}\n",
    "\t f(x,W)= || W*x ||^2.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication(W,x):\n",
    "    q = np.dot(W,x)\n",
    "    return q\n",
    "\n",
    "def prediction(q):\n",
    "    f_1 = q*q\n",
    "    y_pred = np.sum(f_1)\n",
    "   \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the error we need to specify a loss function or objective function and our weight update step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update (W, grad_W, learning_rate):\n",
    "    W = W - learning_rate*grad_W\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_loss(y_pred, y):\n",
    "    error = y_pred-y\n",
    "    loss = np.square(error)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to determine the partial derivatives of our neural network. We aim to determine the gradient of our function \n",
    "$f(x,W)$ dependent from the input values of our network $x$, and the networks parameters $W$.\n",
    "\n",
    "\\begin{align}\n",
    "\t\\frac{\\partial f(x,W)}{\\partial W}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(x,W)}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "By using the chain rule we can easily propagate the error through the local derivatives at each operation and unit.\n",
    "\n",
    "The derivative of the last stage of our unit can be written as: \n",
    "\\begin{align}\n",
    "\tf(q)= ||q||^2 = q_{1}^2+...+q_{n}^2, \\frac{\\partial f(q)}{\\partial q} = 2*q_{i}.\n",
    "\\end{align}\n",
    "\n",
    "For the next node we have two variable inputs and so we have to calculate the local derivatives with respect to both $W$, since this is where we can adjust the weights.\n",
    "\\begin{align}\n",
    "\tq = W*x\t\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial q_{k}}{\\partial W_{i,j}}=x\n",
    "\\end{align}\n",
    "\n",
    "With the chain rule, we can now backpropagate the error to those weights:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial f(x,W)}{\\partial W} = \\frac{\\partial f(q)}{\\partial q} * \\frac{\\partial q}{\\partial W} = 2*q*x^{T}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with the loss function\n",
    "def gradient_loss(y_pred, y):\n",
    "    grad_loss = 2*(y_pred-y)\n",
    "    return grad_loss\n",
    "\n",
    "# Over the square operator\n",
    "def gradient_prediction (q, grad_loss):\n",
    "    grad_q = 2*q*grad_loss\n",
    "    return grad_q\n",
    "\n",
    "# Finally to the multiplication of our inputs\n",
    "def gradient_multiplication (x, grad_q):\n",
    "    if len(x) == 1:\n",
    "        grad_W = grad_q * x.T\n",
    "    else:\n",
    "        grad_W = grad_q.dot(x.T)\n",
    "    return grad_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we bring everything together in our iterative weight update process, in our optimization routine for our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  1.8812865599999997\n",
      "Current prediction:  2.3716\n",
      "Training loss:  0.19077793096573217\n",
      "Current prediction:  1.4367813308347006\n",
      "Training loss:  0.05813516022627628\n",
      "Current prediction:  1.2411123394318015\n",
      "Training loss:  0.021397771751449256\n",
      "Current prediction:  1.14627977218826\n",
      "Training loss:  0.00859828235134978\n",
      "Current prediction:  1.0927269235516297\n",
      "Training loss:  0.003626927562396606\n",
      "Current prediction:  1.0602239783009775\n",
      "Training loss:  0.0015751203015956998\n",
      "Current prediction:  1.0396877852946684\n",
      "Training loss:  0.0006966633103661769\n",
      "Current prediction:  1.0263943802800175\n",
      "Training loss:  0.0003117777091850404\n",
      "Current prediction:  1.0176572282418572\n",
      "Training loss:  0.00014061053282360107\n",
      "Current prediction:  1.0118579312202256\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2\n",
    "\n",
    "weightlist1 = []\n",
    "weightlist2 = []\n",
    "predictionlist = []\n",
    "groundtruthlist = []\n",
    "\n",
    "for t in range(10):\n",
    "    \n",
    "    weightlist1.append(W[0][0])\n",
    "    weightlist2.append(W[0][1])\n",
    "    \n",
    "    # Forward pass\n",
    "    q = multiplication(W, x)\n",
    "    y_pred = prediction(q)\n",
    "    loss = prediction_loss(y_pred,y)\n",
    "    print(\"Training loss: \", loss)\n",
    "    \n",
    "    # Backpropagation\n",
    "    grad_loss = gradient_loss(y_pred, y)\n",
    "    grad_q = gradient_prediction(q, grad_loss)\n",
    "    grad_W = gradient_multiplication(x, grad_q)\n",
    "    W = update(W, grad_W, learning_rate)\n",
    "    predictionlist.append(y_pred)\n",
    "    groundtruthlist.append(y)\n",
    "    print(\"Current prediction: \", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# How do the weights change with respect to the given input?\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(weightlist1, weightlist2)\n",
    "plt.xlabel('Weight 1')\n",
    "plt.ylabel('Weight 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictionlist, groundtruthlist)\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Groundtruth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow bring it on!\n",
    "Now we are going to get our hands dirty with TensorFlow, where most of the hard work is already done for you. This way we can finally focus on \"adding layers\" and \"going deeper\".\n",
    "\n",
    "## Optimization\n",
    "First we concentrate on optimization, this is basically mirroring what we did with numpy a few cell above. This time we will implement a full neural network unit, including an activation function and also we are adding a third input, introducing a bias for the unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# For reproducability\n",
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network architecture\n",
    "n_input = 3\n",
    "n_output = 1\n",
    "n_units = 1\n",
    "\n",
    "# Training parameters\n",
    "n_updates = 10\n",
    "\n",
    "\n",
    "# Define network\n",
    "weights = {\n",
    "    'h1': tf.Variable(np.reshape([np.float32(2.0), np.float32(2.0), np.float32(2.0)], (3, 1))),\n",
    "    # 'h1': tf.Variable(tf.random_normal([n_input, n_units])),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(np.reshape([np.float32(4.0)], (1, 1))),\n",
    "    # 'b1': tf.Variable(tf.random_normal([n_units])),\n",
    "}\n",
    "\n",
    "# This is where we design our unit\n",
    "class unit(tf.keras.Model):\n",
    "    def __init__(self, c_weights, c_biases):\n",
    "        super(unit, self).__init__(name='')\n",
    "        self.c_weights = c_weights\n",
    "        self.c_biases = c_biases\n",
    "\n",
    "    def call(self, x0):\n",
    "        # unit / neuron structure\n",
    "        layer_1 = tf.add(tf.matmul(tf.cast(x0, tf.float32), self.c_weights['h1']), self.c_biases['b1'])\n",
    "        # activation function\n",
    "        y_pred = tf.nn.relu(layer_1)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# Input\n",
    "x = tf.Variable(np.reshape([1.0, 1.0, 3.0], (1, 3)))\n",
    "# predicted output\n",
    "y_pred = unit(weights, biases)\n",
    "# Expected output\n",
    "y_gt = tf.Variable(np.reshape([10.0], (1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# labels, predictions\n",
    "# cost = cost(y_gt, y_pred)\n",
    "\n",
    "# 1. Stochastic gradient descent\n",
    "# opt = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.0001)\n",
    "\n",
    "# 2. Momentum optimizer\n",
    "# opt = tf.train.MomentumOptimizer(learning_rate=0.0001, momentum=0.75)\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.0001, momentum=0.75)\n",
    "\n",
    "# 3. Adaptive momentum optimizer\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# 4. Adaptive momentum optimizer with adjusted initial learning rate\n",
    "# opt = tf.train.AdamOptimizer(learning_rate=0.1)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "# train = opt.minimize(y_gt, y_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable:0' shape=(3, 1) dtype=float32, numpy=\n",
      "array([[1.899997],\n",
      "       [1.899997],\n",
      "       [1.899997]], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, numpy=array([[3.899997]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# Open a GradientTape.\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass.\n",
    "    logits = y_pred(x)\n",
    "    # Loss value for this batch.\n",
    "    loss_value = cost(y_gt, logits)\n",
    "\n",
    "# Get gradients of loss wrt the weights.\n",
    "gradients = tape.gradient(loss_value, y_pred.trainable_weights)\n",
    "\n",
    "# Update the weights of the model.\n",
    "opt.apply_gradients(zip(gradients, y_pred.trainable_weights))\n",
    "\n",
    "\n",
    "print(y_pred.trainable_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the different backpropagation approaches and see whether you can reproduce the findings presented in the lecture.\n",
    "\n",
    "Notice the design of tensorflow, which first designs the model and then calls a session where the placeholders are filled and the forward and backward pass are executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "Next we will get an intuition for regularization approaches (Evaluate the different designs 0. to 2.)  Also we are working with the Keras library, which is based on TensorFlow and introduces a more pythonic feeling. Do you have a notion why I would say so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "\n",
    "  # Design 0. without regularization:\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "\n",
    "  # Design 1. L2 Parameter norm penalty by kernel regularizer:\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "\n",
    "  # Design 2. Dropout:\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "  # tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  # tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training parameters, feel free to play with the different optimizers as well.\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Load training data (reduce training data to 10k samples)\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_validation, y_validation) = mnist.load_data()\n",
    "\n",
    "# Normalize input images (comply with activation function)\n",
    "x_train, x_validation = x_train / 255.0, x_validation / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2155 - accuracy: 0.9352 - val_loss: 0.1285 - val_accuracy: 0.9615\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1067 - accuracy: 0.9692 - val_loss: 0.1075 - val_accuracy: 0.9695\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0761 - accuracy: 0.9776 - val_loss: 0.0839 - val_accuracy: 0.9760\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0614 - accuracy: 0.9828 - val_loss: 0.0911 - val_accuracy: 0.9754\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0911 - accuracy: 0.9754\n",
      "validation accuracy: 0.9753999710083008\n"
     ]
    }
   ],
   "source": [
    "# 3. Augmentation\n",
    "#  sequential_augmentation = Sequential(\n",
    "#        [\n",
    "#            tf.keras.layers.Rescaling(scale=1.0 / 255.0),\n",
    "#            tf.keras.layers.RandomRotation(\n",
    "#                factor=rotation_range, interpolation=interpolation, seed=seed\n",
    "#            ),\n",
    "#            tf.keras.layers.RandomZoom(\n",
    "#                height_factor=zoom_range,\n",
    "#                width_factor=zoom_range,\n",
    "#                fill_mode=\"reflect\",\n",
    "#                interpolation=interpolation,\n",
    "#                seed=seed,\n",
    "#            ),\n",
    "#            tf.keras.layers.RandomFlip(mode=flip, seed=seed),\n",
    "#            tf.keras.layers.RandomHeight(\n",
    "#                factor=height_shift_range, interpolation=interpolation, seed=seed\n",
    "#            ),\n",
    "#            tf.keras.layers.RandomWidth(\n",
    "#                factor=width_shift_range, interpolation=interpolation, seed=seed\n",
    "#            ),\n",
    "#            tf.keras.layers.Resizing(\n",
    "#                resize_height,\n",
    "#                resize_width,\n",
    "#                interpolation=\"bilinear\",\n",
    "#                crop_to_aspect_ratio=False,\n",
    "#            ),\n",
    "#        ]\n",
    "#    )\n",
    "# \n",
    "# \n",
    "# https://keras.io/api/preprocessing/image/\n",
    "# image_generator = (image_dataset_from_directory(img_path, label_path)).map(lambda x: image_augmentation(x))\n",
    "\n",
    "\n",
    "# 4. Early stopping (usually you should monitor the validation accuracy)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                      min_delta=0,\n",
    "                                      patience=1,\n",
    "                                      mode='auto'\n",
    "                                      )\n",
    "\n",
    "# Fit model on training data (with callback)\n",
    "model.fit(x_train, \n",
    "          y_train, \n",
    "          epochs=10, \n",
    "          shuffle=True, \n",
    "          callbacks=[es],\n",
    "          validation_data=(x_validation, y_validation)\n",
    "         )\n",
    "\n",
    "# Fit model on training data (without augmentation)\n",
    "# model.fit(x_train, y_train, epochs=10, shuffle=True)\n",
    "\n",
    "# Evaluate performance on validation set\n",
    "_, validation_acc = model.evaluate(x_validation, y_validation)\n",
    "print('validation accuracy:', validation_acc)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 0. without regularization:\n",
    "# training accuracy: 0.9919\n",
    "# validation accuracy: 0.9801\n",
    "\n",
    "# 1. L2 parameter norm penalties:\n",
    "# training accuracy: 0.9450\n",
    "# validation accuracy: 0.9454\n",
    "\n",
    "# 2. Dropout 0.5:\n",
    "# training accuracy: 0.9616\n",
    "# validation accuracy: 0.9729\n",
    "\n",
    "# 3. Augmentation:\n",
    "# training accuracy: 0.9688\n",
    "# validation accuracy: 0.9778\n",
    "\n",
    "# 4. Early stopping:\n",
    "# training accuracy: 0.9915\n",
    "# validation accuracy: 0.9809\n",
    "\n",
    "# Note: As we observe, regularization does not always yield a beneficial effect. Especially when a model already\n",
    "# performs near to optimal on unobserved data (as our model does on mnist). This also correlates to the large data\n",
    "# available for the task at hand.\n",
    "# Also regularization hampers the training process. Try applying the regularization strategies for a larger number of\n",
    "# training epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps to take it from here\n",
    "\n",
    "- Keep in mind that [MNIST](http://yann.lecun.com/exdb/mnist/) is the \"Hello world\" of machine learning, this is the go-to-dataset if you are in need for a toy problem. Most new ideas in machine learning are presented by the aid of MNIST, make sure that you familiarize yourself with the dataset and its characteristics.\n",
    "- Try different regularization approaches and their influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
